"""
TradingAgents FastAPI Service
æä¾›HTTP APIæ¥å£,åŒ…è£…TradingAgentsæ ¸å¿ƒåŠŸèƒ½
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import json
import os
import uuid
from datetime import datetime
from enum import Enum

# å¯¼å…¥TradingAgentsæ ¸å¿ƒç»„ä»¶
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from dotenv import load_dotenv

# å¯¼å…¥æ•°æ®åº“æœåŠ¡
from db_service import update_task_status, save_report, save_task_message, test_connection

load_dotenv()

# æµ‹è¯•æ•°æ®åº“è¿æ¥
print("ğŸ”„ Testing database connection...")
if test_connection():
    print("âœ… Database ready!")
else:
    print("âš ï¸ Database connection failed, please check configuration")

# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class AnalystType(str, Enum):
    """åˆ†æå¸ˆç±»å‹"""
    MARKET = "market"
    SOCIAL = "social"
    NEWS = "news"
    FUNDAMENTALS = "fundamentals"

class TaskStatus(str, Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class AnalysisRequest(BaseModel):
    """åˆ†æè¯·æ±‚æ¨¡å‹"""
    ticker: str = Field(..., description="è‚¡ç¥¨ä»£ç ", example="NVDA")
    analysis_date: str = Field(..., description="åˆ†ææ—¥æœŸ YYYY-MM-DD", example="2024-05-10")
    selected_analysts: List[AnalystType] = Field(
        default=[AnalystType.MARKET, AnalystType.SOCIAL, AnalystType.NEWS, AnalystType.FUNDAMENTALS],
        description="é€‰æ‹©çš„åˆ†æå¸ˆ"
    )
    research_depth: int = Field(default=1, ge=1, le=5, description="ç ”ç©¶æ·±åº¦(è¾©è®ºè½®æ•°)")
    llm_provider: str = Field(default="openai", description="LLMæä¾›å•†")
    deep_think_llm: str = Field(default="gpt-4o-mini", description="æ·±åº¦æ€è€ƒæ¨¡å‹")
    quick_think_llm: str = Field(default="gpt-4o-mini", description="å¿«é€Ÿæ€è€ƒæ¨¡å‹")
    backend_url: str = Field(default=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1"), description="LLM APIåœ°å€")
    openai_api_key: Optional[str] = Field(default=None, description="OpenAI APIå¯†é’¥(å¯é€‰,ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)")
    alpha_vantage_api_key: Optional[str] = Field(default=None, description="Alpha Vantage APIå¯†é’¥(å¯é€‰)")

class AnalysisResponse(BaseModel):
    """åˆ†æå“åº”æ¨¡å‹"""
    task_id: str
    status: TaskStatus
    message: str
    created_at: str

class TaskDetailResponse(BaseModel):
    """ä»»åŠ¡è¯¦æƒ…å“åº”"""
    task_id: str
    status: TaskStatus
    ticker: str
    analysis_date: str
    selected_analysts: List[str]
    research_depth: int
    final_decision: Optional[str] = None
    created_at: str
    completed_at: Optional[str] = None
    error_message: Optional[str] = None

class ProgressMessage(BaseModel):
    """è¿›åº¦æ¶ˆæ¯æ¨¡å‹"""
    type: str  # status, message, tool_call, report
    timestamp: str
    data: Dict[str, Any]

# ==================== å…¨å±€å˜é‡ ====================

app = FastAPI(
    title="TradingAgents API",
    description="Multi-Agents LLM Financial Trading Framework API",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒéœ€è¦é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# çº¿ç¨‹æ± æ‰§è¡Œå™¨ (ç”¨äºçœŸæ­£çš„åå°å¼‚æ­¥æ‰§è¡Œ)
from concurrent.futures import ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=4)  # æ”¯æŒ4ä¸ªå¹¶å‘ä»»åŠ¡

# ==================== è¾…åŠ©å‡½æ•° ====================

def send_progress_sync(task_id: str, message_type: str, data: Dict[str, Any]):
    """åŒæ­¥ä¿å­˜è¿›åº¦æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    # å°†æ¶ˆæ¯ä¿å­˜åˆ°æ•°æ®åº“
    try:
        save_task_message(task_id, message_type, data)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“å¤±è´¥ {task_id}: {e}")

# ==================== åå°ä»»åŠ¡å¤„ç† ====================

def run_analysis_task_sync(task_id: str, request: AnalysisRequest):
    """åŒæ­¥è¿è¡Œåˆ†æä»»åŠ¡ (åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œ)"""
    ta = None  # åˆå§‹åŒ–ä¸ºNone,ç”¨äºfinallyå—æ¸…ç†

    # ç»Ÿè®¡æŒ‡æ ‡
    tool_call_count = 0
    llm_call_count = 0
    report_count = 0

    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°æ•°æ®åº“
        update_task_status(task_id, "RUNNING")

        send_progress_sync(task_id, "status", {
            "status": "running",
            "message": f"å¼€å§‹åˆ†æ {request.ticker} on {request.analysis_date}"
        })

        # é…ç½®APIå¯†é’¥(å¦‚æœæä¾›)
        original_openai_key = os.environ.get("OPENAI_API_KEY")
        original_av_key = os.environ.get("ALPHA_VANTAGE_API_KEY")

        if request.openai_api_key:
            os.environ["OPENAI_API_KEY"] = request.openai_api_key
        if request.alpha_vantage_api_key:
            os.environ["ALPHA_VANTAGE_API_KEY"] = request.alpha_vantage_api_key

        # åˆ›å»ºé…ç½®
        config = DEFAULT_CONFIG.copy()
        config["deep_think_llm"] = request.deep_think_llm
        config["quick_think_llm"] = request.quick_think_llm
        config["max_debate_rounds"] = request.research_depth
        config["max_risk_discuss_rounds"] = request.research_depth
        config["llm_provider"] = request.llm_provider
        config["backend_url"] = request.backend_url

        # ä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®å”¯ä¸€çš„å†…å­˜å‰ç¼€,é¿å…å¹¶å‘ä»»åŠ¡é—´çš„å†…å­˜æ±¡æŸ“
        config["memory_prefix"] = f"task_{task_id[:8]}_"

        print(f"ğŸš€ åˆå§‹åŒ– TradingAgentsGraph, åˆ†æå¸ˆ: {[a.value for a in request.selected_analysts]}")
        print(f"ğŸ“ å†…å­˜å‰ç¼€: {config['memory_prefix']}")

        # åˆå§‹åŒ–TradingAgents
        ta = TradingAgentsGraph(
            selected_analysts=[analyst.value for analyst in request.selected_analysts],
            debug=True,
            config=config
        )

        send_progress_sync(task_id, "status", {
            "status": "running",
            "message": "TradingAgents åˆå§‹åŒ–å®Œæˆ"
        })

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        init_state = ta.propagator.create_initial_state(
            request.ticker,
            request.analysis_date
        )
        args = ta.propagator.get_graph_args()

        print(f"ğŸ“Š å¼€å§‹æµå¼æ‰§è¡Œåˆ†æ...")

        # æµå¼æ‰§è¡Œåˆ†æ
        chunk_count = 0
        final_chunk = None

        for chunk in ta.graph.stream(init_state, **args):
            chunk_count += 1
            final_chunk = chunk

            # æ£€æµ‹å½“å‰æ´»è·ƒçš„èŠ‚ç‚¹/Agent
            # LangGraphçš„chunkå¯èƒ½åŒ…å«èŠ‚ç‚¹åç§°ä¿¡æ¯
            chunk_keys = list(chunk.keys())
            for key in chunk_keys:
                # å‘é€èŠ‚ç‚¹æ¿€æ´»ä¿¡æ¯
                if key not in ["messages"] and chunk.get(key):
                    # æ˜ å°„èŠ‚ç‚¹ååˆ°Agent
                    agent_mapping = {
                        "market_analyst": "market",
                        "social_analyst": "social",
                        "news_analyst": "news",
                        "fundamentals_analyst": "fundamentals",
                        "bull_researcher": "bull",
                        "bear_researcher": "bear",
                        "research_manager": "research-manager",
                        "trader": "trader",
                        "risky_analyst": "risk-risky",
                        "safe_analyst": "risk-safe",
                        "neutral_analyst": "risk-neutral",
                        "risk_manager": "risk-manager"
                    }

                    if key in agent_mapping:
                        send_progress_sync(task_id, "agent_status", {
                            "agent": agent_mapping[key],
                            "status": "running"
                        })

            # å¤„ç†æ¶ˆæ¯
            if len(chunk.get("messages", [])) > 0:
                last_message = chunk["messages"][-1]
                llm_call_count += 1  # æ¯ä¸ªæ¶ˆæ¯éƒ½æ˜¯ä¸€æ¬¡LLMè°ƒç”¨

                # æå–å†…å®¹
                if hasattr(last_message, "content"):
                    content = last_message.content
                    if isinstance(content, list):
                        # å¤„ç†Anthropicæ ¼å¼
                        text_parts = []
                        for item in content:
                            if isinstance(item, dict) and item.get('type') == 'text':
                                text_parts.append(item.get('text', ''))
                        content = ' '.join(text_parts)

                    send_progress_sync(task_id, "message", {
                        "content": str(content),  # å‘é€å®Œæ•´å†…å®¹
                        "stats": {
                            "tool_calls": tool_call_count,
                            "llm_calls": llm_call_count,
                            "reports": report_count
                        }
                    })

                # å¤„ç†å·¥å…·è°ƒç”¨
                if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                    for tool_call in last_message.tool_calls:
                        tool_call_count += 1  # ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•°

                        tool_name = tool_call.get("name") if isinstance(tool_call, dict) else getattr(tool_call, "name", "unknown")
                        tool_args = tool_call.get("args") if isinstance(tool_call, dict) else getattr(tool_call, "args", {})

                        send_progress_sync(task_id, "tool_call", {
                            "tool_name": tool_name,
                            "args": tool_args,
                            "stats": {
                                "tool_calls": tool_call_count,
                                "llm_calls": llm_call_count,
                                "reports": report_count
                            }
                        })

            # å¤„ç†æŠ¥å‘Š
            report_types = [
                "market_report", "sentiment_report", "news_report",
                "fundamentals_report", "investment_plan",
                "trader_investment_plan", "final_trade_decision"
            ]

            for report_type in report_types:
                if report_type in chunk and chunk[report_type]:
                    report_count += 1  # ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆæ¬¡æ•°

                    send_progress_sync(task_id, "report", {
                        "report_type": report_type,
                        "content": chunk[report_type],
                        "stats": {
                            "tool_calls": tool_call_count,
                            "llm_calls": llm_call_count,
                            "reports": report_count
                        }
                    })

                    # ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“
                    save_report(task_id, report_type, chunk[report_type])

        print(f"âœ… åˆ†æå®Œæˆ,å…±å¤„ç† {chunk_count} ä¸ªchunk")

        # è·å–æœ€ç»ˆå†³ç­–
        if final_chunk and "final_trade_decision" in final_chunk:
            decision = ta.process_signal(final_chunk["final_trade_decision"])
        else:
            decision = "UNKNOWN"

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆï¼ˆå†™å…¥æ•°æ®åº“ï¼‰
        update_task_status(task_id, "COMPLETED", final_decision=decision)

        send_progress_sync(task_id, "status", {
            "status": "completed",
            "decision": decision,
            "message": f"åˆ†æå®Œæˆ! å†³ç­–: {decision}"
        })

        # æ¢å¤åŸå§‹APIå¯†é’¥
        if original_openai_key:
            os.environ["OPENAI_API_KEY"] = original_openai_key
        if original_av_key:
            os.environ["ALPHA_VANTAGE_API_KEY"] = original_av_key

    except Exception as e:
        print(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆå†™å…¥æ•°æ®åº“ï¼‰
        update_task_status(task_id, "FAILED", error_message=str(e))

        send_progress_sync(task_id, "status", {
            "status": "failed",
            "error": str(e)
        })
    finally:
        # æ¸…ç†å†…å­˜é›†åˆ,é‡Šæ”¾èµ„æº,é¿å…å†…å­˜æ³„æ¼
        if ta is not None:
            try:
                ta.cleanup_memory()
                print(f"ğŸ§¹ ä»»åŠ¡ {task_id[:8]} å†…å­˜å·²æ¸…ç†")
            except Exception as cleanup_error:
                print(f"âš ï¸ æ¸…ç†ä»»åŠ¡ {task_id[:8]} å†…å­˜æ—¶å‡ºé”™: {cleanup_error}")

# ==================== APIç«¯ç‚¹ ====================

@app.get("/", tags=["å¥åº·æ£€æŸ¥"])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "TradingAgents API",
        "status": "running",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/api/v1/health", tags=["å¥åº·æ£€æŸ¥"])
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    from database import get_db_session, Task

    db = get_db_session()
    try:
        active_tasks = db.query(Task).filter(Task.status.in_(["PENDING", "RUNNING"])).count()
    finally:
        db.close()

    return {
        "status": "healthy",
        "service": "TradingAgents API",
        "timestamp": datetime.now().isoformat(),
        "active_tasks": active_tasks
    }

@app.post("/api/v1/analysis/start", response_model=AnalysisResponse, tags=["åˆ†æ"])
async def start_analysis(
    request: AnalysisRequest
):
    """
    å¯åŠ¨æ–°çš„åˆ†æä»»åŠ¡
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹ä»…ç”¨äº Python å†…éƒ¨æµ‹è¯•ï¼Œå®é™…åº”è¯¥ç”± Java ç«¯è°ƒç”¨

    - **ticker**: è‚¡ç¥¨ä»£ç ,å¦‚ NVDA, AAPL, TSLA
    - **analysis_date**: åˆ†ææ—¥æœŸ,æ ¼å¼ YYYY-MM-DD
    - **selected_analysts**: é€‰æ‹©çš„åˆ†æå¸ˆåˆ—è¡¨
    - **research_depth**: ç ”ç©¶æ·±åº¦(è¾©è®ºè½®æ•°), 1-5
    """
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())

    # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œä»»åŠ¡ (çœŸæ­£çš„å¹¶å‘)
    loop = asyncio.get_event_loop()
    loop.run_in_executor(executor, run_analysis_task_sync, task_id, request)

    print(f"âœ¨ åˆ›å»ºåˆ†æä»»åŠ¡: {task_id} - {request.ticker} on {request.analysis_date}")

    return AnalysisResponse(
        task_id=task_id,
        status=TaskStatus.PENDING,
        message=f"åˆ†æä»»åŠ¡å·²åˆ›å»º: {request.ticker}",
        created_at=datetime.now().isoformat()
    )

@app.get("/api/v1/analysis/{task_id}", response_model=TaskDetailResponse, tags=["åˆ†æ"])
async def get_task_detail(task_id: str):
    """
    è·å–ä»»åŠ¡è¯¦æƒ…ï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰
    """
    from db_service import get_task_by_uuid
    import json

    task = get_task_by_uuid(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    # è§£æ JSON å­—æ®µ
    selected_analysts = json.loads(task.selected_analysts) if task.selected_analysts else []

    return TaskDetailResponse(
        task_id=task.task_id,
        status=task.status,
        ticker=task.ticker,
        analysis_date=task.analysis_date.strftime("%Y-%m-%d"),
        selected_analysts=selected_analysts,
        research_depth=task.research_depth,
        final_decision=task.final_decision,
        created_at=task.created_at.isoformat(),
        completed_at=task.completed_at.isoformat() if task.completed_at else None,
        error_message=task.error_message
    )

@app.get("/api/v1/analysis/{task_id}/reports", tags=["åˆ†æ"])
async def get_task_reports(task_id: str):
    """
    è·å–ä»»åŠ¡çš„æ‰€æœ‰æŠ¥å‘Šï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰
    """
    from db_service import get_task_by_uuid
    from database import get_db_session, Report

    task = get_task_by_uuid(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    # è·å–æŠ¥å‘Š
    db = get_db_session()
    try:
        reports = db.query(Report).filter(Report.task_id == task.id).all()
        reports_dict = {}
        for report in reports:
            reports_dict[report.report_type] = report.content
        return {
            "task_id": task_id,
            "status": task.status,
            "reports": reports_dict
        }
    finally:
        db.close()

@app.get("/api/v1/tasks", tags=["åˆ†æ"])
async def list_tasks(status: Optional[TaskStatus] = None, limit: int = 20):
    """
    åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹è¿”å› Python å†…éƒ¨æ•°æ®ï¼ŒJava ç«¯åº”ä½¿ç”¨è‡ªå·±çš„ API
    """
    from database import get_db_session, Task
    import json

    db = get_db_session()
    try:
        query = db.query(Task)

        if status:
            query = query.filter(Task.status == status.value.upper())

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åº
        query = query.order_by(Task.created_at.desc()).limit(limit)
        tasks = query.all()

        task_list = []
        for task in tasks:
            selected_analysts = json.loads(task.selected_analysts) if task.selected_analysts else []
            task_list.append({
                "task_id": task.task_id,
                "status": task.status,
                "ticker": task.ticker,
                "analysis_date": task.analysis_date.strftime("%Y-%m-%d"),
                "selected_analysts": selected_analysts,
                "research_depth": task.research_depth,
                "final_decision": task.final_decision,
                "created_at": task.created_at.isoformat(),
                "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                "error_message": task.error_message
            })

        return {
            "total": len(task_list),
            "tasks": task_list
        }
    finally:
        db.close()

# ==================== å¯åŠ¨é…ç½® ====================

if __name__ == "__main__":
    import uvicorn

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         TradingAgents FastAPI Service                     â•‘
    â•‘                                                           â•‘
    â•‘         http://localhost:8000/docs                        â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
